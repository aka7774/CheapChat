{
    "gpt-4o": {
		"provider": "openai",
		"endpoint": "",
		"model": "gpt-4o",
		"qtype": "",
		"dtype": "",
		"inst_template": "",
		"chat_template": "",
		"input_tokens": 128000,
		"max_new_tokens": 1024,
		"temperature": 1.0,
		"top_p": 0.9,
		"top_k": 40,
		"repetition_penalty": 1.1
	},
    "claude-3-5-sonnet-20240620": {
		"provider": "anthropic",
		"endpoint": "",
		"model": "claude-3-5-sonnet-20240620",
		"qtype": "",
		"dtype": "",
		"inst_template": "",
		"chat_template": "",
		"input_tokens": 128000,
		"max_new_tokens": 1024,
		"temperature": 1.0,
		"top_p": 0.9,
		"top_k": 40,
		"repetition_penalty": 1.1
	},
	"Local-Novel-LLM-project/Ninja-v1-128k": {
		"provider": "trllm",
		"endpoint": "",
		"model": "Local-Novel-LLM-project/Ninja-v1-128k",
		"qtype": "",
		"dtype": "4bit",
		"inst_template": "{instruction}\n\n{input}\n",
		"chat_template": "",
		"input_tokens": 8192,
		"max_new_tokens": 128,
		"temperature": 0.1,
		"top_p": 0.9,
		"top_k": 40,
		"repetition_penalty": 1.1
	},
    "DataPilot/ArrowPro-7B-KUJIRA": {
		"provider": "trllm",
		"endpoint": "",
		"model": "DataPilot/ArrowPro-7B-KUJIRA",
		"qtype": "",
		"dtype": "4bit",
		"inst_template": "{instruction}\n\n{input}\n",
		"chat_template": "",
		"input_tokens": 8192,
		"max_new_tokens": 128,
		"temperature": 0.1,
		"top_p": 0.9,
		"top_k": 40,
		"repetition_penalty": 1.1
	},
    "DataPilot/ArrowPro-7B-RobinHood": {
		"provider": "trllm",
		"endpoint": "",
		"model": "DataPilot/ArrowPro-7B-RobinHood",
		"qtype": "",
		"dtype": "4bit",
		"inst_template": "{instruction}\n\n{input}\n",
		"chat_template": "",
		"input_tokens": 8192,
		"max_new_tokens": 128,
		"temperature": 0.1,
		"top_p": 0.9,
		"top_k": 40,
		"repetition_penalty": 1.1
	},
    "lightblue/qarasu-14B-chat-plus-unleashed": {
		"provider": "trllm",
		"endpoint": "",
		"model": "lightblue/qarasu-14B-chat-plus-unleashed",
		"qtype": "",
		"dtype": "4bit",
		"inst_template": "{instruction}\n\n{input}\n",
		"chat_template": "",
		"input_tokens": 256,
		"max_new_tokens": 128,
		"temperature": 0.1,
		"top_p": 0.9,
		"top_k": 40,
		"repetition_penalty": 1.1
	},
    "elyza/ELYZA-japanese-Llama-2-13b-fast-instruct": {
		"provider": "trllm",
		"endpoint": "",
		"model": "elyza/ELYZA-japanese-Llama-2-13b-fast-instruct",
		"qtype": "",
		"dtype": "4bit",
		"inst_template": "[INST] <<SYS>>\n{instruction}\n<</SYS>>\n{input} [/INST]",
		"chat_template": "",
		"input_tokens": 256,
		"max_new_tokens": 128,
		"temperature": 1.0,
		"top_p": 0.9,
		"top_k": 40,
		"repetition_penalty": 1.1
	},
    "tokyotech-llm/Swallow-13b-instruct-hf": {
		"provider": "trllm",
		"endpoint": "",
		"model": "tokyotech-llm/Swallow-13b-instruct-hf",
		"qtype": "",
		"dtype": "4bit",
		"inst_template": "以下に、あるタスクを説明する指示があり、それに付随する入力が更なる文脈を提供しています。リクエストを適切に完了するための回答を記述してください。\n\n### 指示:\n{instruction}\n\n### 入力:\n{input}\n\n### 応答:",
		"chat_template": "",
		"input_tokens": 256,
		"max_new_tokens": 128,
		"temperature": 0.99,
		"top_p": 0.95,
		"top_k": 40,
		"repetition_penalty": 1.1
	},
    "rinna/nekomata-14b-instruction": {
		"provider": "trllm",
		"endpoint": "",
		"model": "rinna/nekomata-14b-instruction",
		"qtype": "",
		"dtype": "4bit",
		"inst_template": "以下は、タスクを説明する指示と、文脈のある入力の組み合わせです。要求を適切に満たす応答を書きなさい。\n\n### 指示:\n{instruction}\n\n### 入力:\n{input}\n\n### 応答:",
		"chat_template": "",
		"input_tokens": 256,
		"max_new_tokens": 128,
		"temperature": 0.5,
		"top_p": 0.95,
		"top_k": 40,
		"repetition_penalty": 1.1
	},
    "cyberagent/calm2-7b-chat": {
		"provider": "trllm",
		"endpoint": "",
		"model": "cyberagent/calm2-7b-chat",
		"qtype": "",
		"dtype": "4bit",
		"inst_template": "{instruction}\nUSER: {input}\nASSISTANT: ",
		"chat_template": "",
		"input_tokens": 256,
		"max_new_tokens": 128,
		"temperature": 0.8,
		"top_p": 0.9,
		"top_k": 40,
		"repetition_penalty": 1.1
	},
    "llm-jp/llm-jp-13b-instruct-full-jaster-dolly-oasst-v1.0": {
		"provider": "trllm",
		"endpoint": "",
		"model": "llm-jp/llm-jp-13b-instruct-full-jaster-dolly-oasst-v1.0",
		"qtype": "",
		"dtype": "4bit",
		"inst_template": "{instruction}\n{input}\n### 回答：",
		"chat_template": "",
		"input_tokens": 256,
		"max_new_tokens": 128,
		"temperature": 0.7,
		"top_p": 0.95,
		"top_k": 40,
		"repetition_penalty": 1.1
	},
    "stockmark/stockmark-13b-instruct": {
		"provider": "trllm",
		"endpoint": "",
		"model": "stockmark/stockmark-13b-instruct",
		"qtype": "",
		"dtype": "4bit",
		"inst_template": "{instruction}\n\n### Input:\n{input}\n\n### Output:",
		"chat_template": "",
		"input_tokens": 256,
		"max_new_tokens": 128,
		"temperature": 0.7,
		"top_p": 0.9,
		"top_k": 40,
		"repetition_penalty": 1.1
	},
    "gpt-3.5-turbo-0125": {
		"provider": "openai",
		"endpoint": "",
		"model": "gpt-3.5-turbo-0125",
		"qtype": "",
		"dtype": "",
		"inst_template": "",
		"chat_template": "",
		"input_tokens": 16385,
		"max_new_tokens": 128,
		"temperature": 1.0,
		"top_p": 0.9,
		"top_k": 40,
		"repetition_penalty": 1.1
	},
    "gpt-4-0125-preview": {
		"provider": "openai",
		"endpoint": "",
		"model": "gpt-4-0125-preview",
		"qtype": "",
		"dtype": "",
		"inst_template": "",
		"chat_template": "",
		"input_tokens": 128000,
		"max_new_tokens": 128,
		"temperature": 1.0,
		"top_p": 0.9,
		"top_k": 40,
		"repetition_penalty": 1.1
	},
    "gpt-4-turbo-2024-04-09": {
		"provider": "openai",
		"endpoint": "",
		"model": "gpt-4-turbo-2024-04-09",
		"qtype": "",
		"dtype": "",
		"inst_template": "",
		"chat_template": "",
		"input_tokens": 128000,
		"max_new_tokens": 128,
		"temperature": 1.0,
		"top_p": 0.9,
		"top_k": 40,
		"repetition_penalty": 1.1
	},
    "gpt-4-0613": {
		"provider": "openai",
		"endpoint": "",
		"model": "gpt-4-0613",
		"qtype": "",
		"dtype": "",
		"inst_template": "",
		"chat_template": "",
		"input_tokens": 8192,
		"max_new_tokens": 128,
		"temperature": 1.0,
		"top_p": 0.9,
		"top_k": 40,
		"repetition_penalty": 1.1
	},
    "gpt-4-32k-0613": {
		"provider": "openai",
		"endpoint": "",
		"model": "gpt-4-32k-0613",
		"qtype": "",
		"dtype": "",
		"inst_template": "",
		"chat_template": "",
		"input_tokens": 32768,
		"max_new_tokens": 128,
		"temperature": 1.0,
		"top_p": 0.9,
		"top_k": 40,
		"repetition_penalty": 1.1
	},
    "claude-3-opus-20240229": {
		"provider": "anthropic",
		"endpoint": "",
		"model": "claude-3-opus-20240229",
		"qtype": "",
		"dtype": "",
		"inst_template": "",
		"chat_template": "",
		"input_tokens": 128000,
		"max_new_tokens": 1024,
		"temperature": 1.0,
		"top_p": 0.9,
		"top_k": 40,
		"repetition_penalty": 1.1
	},
    "claude-3-sonnet-20240229": {
		"provider": "anthropic",
		"endpoint": "",
		"model": "claude-3-sonnet-20240229",
		"qtype": "",
		"dtype": "",
		"inst_template": "",
		"chat_template": "",
		"input_tokens": 4096,
		"max_new_tokens": 128,
		"temperature": 1.0,
		"top_p": 0.9,
		"top_k": 40,
		"repetition_penalty": 1.1
	},
    "claude-3-haiku-20240307": {
		"provider": "anthropic",
		"endpoint": "",
		"model": "claude-3-haiku-20240307",
		"qtype": "",
		"dtype": "",
		"inst_template": "",
		"chat_template": "",
		"input_tokens": 4096,
		"max_new_tokens": 128,
		"temperature": 1.0,
		"top_p": 0.9,
		"top_k": 40,
		"repetition_penalty": 1.1
	},
    "gemini-1.5-pro-latest": {
		"provider": "googleai",
		"endpoint": "",
		"model": "gemini-1.5-pro-latest",
		"qtype": "",
		"dtype": "",
		"inst_template": "",
		"chat_template": "",
		"input_tokens": 1000000,
		"max_new_tokens": 128,
		"temperature": 1.0,
		"top_p": 0.9,
		"top_k": 40,
		"repetition_penalty": 1.1
	},
    "cyberagent/calm2-7b-chat-dpo-experimental": {
		"provider": "trllm",
		"endpoint": "",
		"model": "cyberagent/calm2-7b-chat-dpo-experimental",
		"qtype": "",
		"dtype": "4bit",
		"inst_template": "{instruction}\nUSER: {input}\nASSISTANT: ",
		"chat_template": "",
		"input_tokens": 4096,
		"max_new_tokens": 128,
		"temperature": 1.0,
		"top_p": 0.9,
		"top_k": 40,
		"repetition_penalty": 1.1
	},
    "Sdff-Ltba/LightChatAssistant-2x7B": {
		"provider": "trllm",
		"endpoint": "",
		"model": "Sdff-Ltba/LightChatAssistant-2x7B",
		"qtype": "",
		"dtype": "4bit",
		"inst_template": "{instruction}\n\n{input}\n",
		"chat_template": "",
		"input_tokens": 4096,
		"max_new_tokens": 128,
		"temperature": 1.0,
		"top_p": 0.9,
		"top_k": 40,
		"repetition_penalty": 1.1
	},
    "Sdff-Ltba/chatntq_chatvector-MoE-Antler_chatvector-2x7B": {
		"provider": "trllm",
		"endpoint": "",
		"model": "Sdff-Ltba/chatntq_chatvector-MoE-Antler_chatvector-2x7B",
		"qtype": "",
		"dtype": "4bit",
		"inst_template": "{instruction}\n\n{input}\n",
		"chat_template": "",
		"input_tokens": 4096,
		"max_new_tokens": 128,
		"temperature": 1.0,
		"top_p": 0.9,
		"top_k": 40,
		"repetition_penalty": 1.1
	}
}
